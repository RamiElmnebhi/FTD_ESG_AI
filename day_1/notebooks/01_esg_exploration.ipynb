{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESG Data Exploration\n",
    "\n",
    "This notebook introduces key ESG concepts and provides interactive examples for exploring ESG data.\n",
    "\n",
    "## 1. Understanding ESG Components\n",
    "\n",
    "ESG stands for Environmental, Social, and Governance:\n",
    "\n",
    "- **Environmental**: Climate change, carbon emissions, water usage, waste management\n",
    "- **Social**: Employee relations, diversity, human rights, community impact\n",
    "- **Governance**: Board composition, executive compensation, shareholder rights\n",
    "\n",
    "Let's explore these components in real ESG reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pdfplumber\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_theme()  # This is the recommended way to set the style in newer versions\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ESG Keywords and Metrics\n",
    "\n",
    "Let's define some common ESG keywords to look for in reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_keywords = {\n",
    "    'environmental': [\n",
    "        'carbon emissions', 'climate change', 'renewable energy', 'waste management',\n",
    "        'water consumption', 'biodiversity', 'pollution', 'recycling'\n",
    "    ],\n",
    "    'social': [\n",
    "        'diversity', 'inclusion', 'human rights', 'employee safety', 'community',\n",
    "        'labor practices', 'data privacy', 'health'\n",
    "    ],\n",
    "    'governance': [\n",
    "        'board diversity', 'executive compensation', 'shareholder rights',\n",
    "        'ethics', 'compliance', 'transparency', 'risk management'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing ESG Report Content\n",
    "\n",
    "Let's analyze our sample ESG report to see the frequency of ESG-related terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_esg_content(pdf_path):\n",
    "    \"\"\"Analyze ESG content in a PDF report.\"\"\"\n",
    "    # Extract text\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = '\\n'.join(page.extract_text() for page in pdf.pages)\n",
    "    \n",
    "    # Count ESG keywords\n",
    "    keyword_counts = {category: {} for category in esg_keywords}\n",
    "    for category, words in esg_keywords.items():\n",
    "        for word in words:\n",
    "            count = len(re.findall(word, text.lower()))\n",
    "            if count > 0:\n",
    "                keyword_counts[category][word] = count\n",
    "    \n",
    "    return keyword_counts\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = os.path.join('..', 'data', 'totalenergies_sustainability-climate-2024-progress-report_2024_en_pdf.pdf')\n",
    "\n",
    "# Analyze content if file exists\n",
    "if os.path.exists(pdf_path):\n",
    "    keyword_counts = analyze_esg_content(pdf_path)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, (category, counts) in enumerate(keyword_counts.items()):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        if counts:  # If we found any keywords\n",
    "            plt.bar(counts.keys(), counts.values())\n",
    "            plt.title(f'{category.capitalize()} Keywords')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"PDF file not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise: ESG Metric Extraction\n",
    "\n",
    "Try to extract specific ESG metrics from the report. Here's an example pattern to find carbon emission values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_esg_metrics(text):\n",
    "    \"\"\"Extract various ESG metrics from text.\"\"\"\n",
    "    metrics = {\n",
    "        'carbon_metrics': [],\n",
    "        'renewable_metrics': [],\n",
    "    }\n",
    "    \n",
    "    # Carbon metrics patterns\n",
    "    carbon_patterns = [\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:MtCO2e|million tons of CO2|Million tons CO2)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:gCO2/kWh|gCO2e/kWh)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:%|percent)\\s*(?:reduction|decrease)\\s*(?:in|of)\\s*(?:emissions|carbon)'\n",
    "    ]\n",
    "    \n",
    "    # Renewable energy metrics patterns\n",
    "    renewable_patterns = [\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:GW|gigawatts?)\\s*(?:of|installed)?\\s*(?:renewable|solar|wind)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:%|percent)\\s*(?:renewable|solar|wind)\\s*(?:energy|capacity)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:TWh|terawatt hours?)\\s*(?:of|generated)?\\s*(?:renewable|solar|wind)'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Extract metrics using patterns\n",
    "    for pattern in carbon_patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        metrics['carbon_metrics'].extend([match.group() for match in matches])\n",
    "    \n",
    "    for pattern in renewable_patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        metrics['renewable_metrics'].extend([match.group() for match in matches])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage (if PDF exists)\n",
    "if os.path.exists(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = '\\n'.join(page.extract_text() for page in pdf.pages)\n",
    "        metrics = extract_esg_metrics(text)\n",
    "        \n",
    "        print(\"ESG Metrics Found:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for category, found_metrics in metrics.items():\n",
    "            if found_metrics:\n",
    "                print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "                for metric in found_metrics[:5]:  # Show first 5 matches\n",
    "                    print(f\"- {metric}\")\n",
    "            else:\n",
    "                print(f\"\\nNo {category.replace('_', ' ')} found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "Try these exercises to practice ESG data analysis:\n",
    "\n",
    "1. Create a function to extract diversity metrics (e.g., percentage of women in workforce)\n",
    "2. Create a function to extract investment metrics (e.g., amounts invested in renewable energy)\n",
    "3. Create a function to extract water consumption metrics (e.g., water withdrawal, water recycling rates)\n",
    "4. Create a function to extract waste management metrics (e.g., waste generation, recycling rates)\n",
    "\n",
    "Example solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_diversity_metrics(text):\n",
    "    \"\"\"Extract diversity-related metrics from text.\"\"\"\n",
    "    # Pattern for percentages near diversity-related words\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?%)\\s*(?:women|diverse|minority|representation)'\n",
    "    matches = re.finditer(pattern, text.lower())\n",
    "    return [match.group() for match in matches]\n",
    "\n",
    "# Example solution for exercise 2:\n",
    "def extract_investment_metrics(text):\n",
    "    \"\"\"Extract investment-related metrics from text.\"\"\"\n",
    "    # Pattern for amounts invested in renewable energy\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:USD|EUR|€|\\$)\\s*(?:invested|investment|spent)\\s*(?:in|for)\\s*(?:renewable|solar|wind|clean)'\n",
    "    matches = re.finditer(pattern, text.lower())\n",
    "    return [match.group() for match in matches]\n",
    "\n",
    "# Example solution for exercise 3:\n",
    "def extract_water_metrics(text):\n",
    "    \"\"\"Extract water-related metrics from text.\"\"\"\n",
    "    # Pattern for water consumption and recycling\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:million|billion)?\\s*(?:m3|cubic meters?|liters?)\\s*(?:of)?\\s*(?:water|wastewater)\\s*(?:withdrawn|consumed|recycled|reused)'\n",
    "    matches = re.finditer(pattern, text.lower())\n",
    "    return [match.group() for match in matches]\n",
    "\n",
    "# Example solution for exercise 4:\n",
    "def extract_waste_metrics(text):\n",
    "    \"\"\"Extract waste management metrics from text.\"\"\"\n",
    "    # Pattern for waste generation and recycling\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:million|thousand)?\\s*(?:tons?|tonnes?)\\s*(?:of)?\\s*(?:waste|hazardous waste)\\s*(?:generated|recycled|disposed)'\n",
    "    matches = re.finditer(pattern, text.lower())\n",
    "    return [match.group() for match in matches]\n",
    "\n",
    "# Test the functions with the PDF\n",
    "if os.path.exists(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = '\\n'.join(page.extract_text() for page in pdf.pages)\n",
    "        \n",
    "        print(\"Diversity Metrics:\")\n",
    "        print(extract_diversity_metrics(text))\n",
    "        \n",
    "        print(\"\\nInvestment Metrics:\")\n",
    "        print(extract_investment_metrics(text))\n",
    "        \n",
    "        print(\"\\nWater Metrics:\")\n",
    "        print(extract_water_metrics(text))\n",
    "        \n",
    "        print(\"\\nWaste Metrics:\")\n",
    "        print(extract_waste_metrics(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced NLP Analysis\n",
    "\n",
    "Let's explore more sophisticated NLP techniques to analyze ESG content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional NLP libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download spaCy model (run this once)\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: remove numbers, special chars, lemmatize\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Remove numbers and special characters, keep only words\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize sentences and words, lemmatize\n",
    "    words = text.lower().split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if len(word) > 3]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500):\n",
    "    \"\"\"Splits text into smaller chunks for better topic modeling\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        if len(' '.join(current_chunk)) > chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def perform_topic_modeling(text, n_topics=5, method=\"nmf\"):\n",
    "    \"\"\"Perform topic modeling on a document split into chunks\"\"\"\n",
    "    \n",
    "    # Preprocess text\n",
    "    text = preprocess_text(text)\n",
    "    chunks = split_text_into_chunks(text)\n",
    "\n",
    "    # Use TF-IDF vectorization for better results\n",
    "    vectorizer = TfidfVectorizer(\n",
    "    min_df=1,  # Allow words that appear in at least one chunk\n",
    "    stop_words='english', \n",
    "    max_features=1000\n",
    "    )\n",
    "    \n",
    "    doc_term_matrix = vectorizer.fit_transform(chunks)\n",
    "    \n",
    "    if method == \"lda\":\n",
    "        model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=42,\n",
    "            learning_method='batch',\n",
    "            max_iter=50\n",
    "        )\n",
    "    else:  # Default to NMF\n",
    "        model = NMF(n_components=n_topics, random_state=42, init=\"random\")\n",
    "    \n",
    "    model_output = model.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Extract top words per topic\n",
    "    topic_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[:-10 - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topic_words[f\"Topic {topic_idx + 1}\"] = top_words\n",
    "\n",
    "    return topic_words\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    \"\"\"Extract and categorize named entities using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = []\n",
    "        if ent.text not in entities[ent.label_]:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = os.path.join('..', 'data', 'totalenergies_sustainability-climate-2024-progress-report_2024_en_pdf.pdf')\n",
    "\n",
    "# 1. Topic Modeling\n",
    "if os.path.exists(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "        \n",
    "        # Perform topic modeling\n",
    "        topics = perform_topic_modeling(text, n_topics=5, method=\"nmf\")\n",
    "        \n",
    "        print(\"\\nImproved Topic Modeling Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        for topic, words in topics.items():\n",
    "            print(f\"\\n{topic}:\")\n",
    "            print(\", \".join(words))\n",
    "    \n",
    "    # 2. Named Entity Recognition\n",
    "    print(\"\\nNamed Entity Recognition:\")\n",
    "    print(\"-\" * 50)\n",
    "    entities = extract_named_entities(text[:100000])  # Process first 100K chars\n",
    "    \n",
    "    # Visualize entity distribution\n",
    "    entity_counts = {k: len(v) for k, v in entities.items()}\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(entity_counts.keys(), entity_counts.values())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Distribution of Named Entity Types')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Semantic Analysis\n",
    "\n",
    "Let's analyze the semantic similarity between different sections of the report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs_using_sentences(text, min_length=200):\n",
    "    \"\"\"Extracts paragraphs by splitting based on sentences instead of newlines.\"\"\"\n",
    "    sentences = sent_tokenize(text)  # Use sentence tokenizer\n",
    "\n",
    "    paragraphs = []\n",
    "    current_paragraph = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_paragraph.append(sentence)\n",
    "        \n",
    "        # Group sentences into a paragraph when it reaches a reasonable length\n",
    "        if len(\" \".join(current_paragraph)) > min_length:\n",
    "            paragraphs.append(\" \".join(current_paragraph))\n",
    "            current_paragraph = []\n",
    "\n",
    "    if len(\" \".join(current_paragraph)) > min_length:\n",
    "        paragraphs.append(\" \".join(current_paragraph))\n",
    "\n",
    "    print(f\"\\n✅ Extracted {len(paragraphs)} paragraphs using sentence tokenization.\")\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "\n",
    "def analyze_semantic_similarity(text, min_length=200):\n",
    "    \"\"\"Analyze semantic similarity between paragraphs using SBERT embeddings.\"\"\"\n",
    "    \n",
    "    # Step 1: Extract paragraphs using sentence tokenization instead of newlines\n",
    "    paragraphs = extract_paragraphs_using_sentences(text, min_length)\n",
    "\n",
    "    # Step 2: Check if we have enough paragraphs\n",
    "    if len(paragraphs) < 2:\n",
    "        print(\"\\n❌ Not enough paragraphs for semantic similarity analysis.\")\n",
    "        return None, None\n",
    "\n",
    "    # Step 3: Load SBERT model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Step 4: Compute embeddings\n",
    "    embeddings = model.encode(paragraphs, convert_to_tensor=True)\n",
    "\n",
    "    # Step 5: Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings.cpu().numpy())\n",
    "\n",
    "    # Step 6: Visualize similarity matrix\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(similarity_matrix[:20, :20], cmap='YlOrRd', annot=True)\n",
    "    plt.title('Semantic Similarity Between First 20 Paragraphs')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return similarity_matrix, paragraphs\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = os.path.join('..', 'data', 'totalenergies_sustainability-climate-2024-progress-report_2024_en_pdf.pdf')\n",
    "\n",
    "if os.path.exists(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "\n",
    "    print(\"Analyzing semantic similarity using SBERT...\")\n",
    "    \n",
    "    similarity_matrix, paragraphs = analyze_semantic_similarity(text)\n",
    "    \n",
    "    if paragraphs is None:\n",
    "        print(\"Error: Not enough paragraphs extracted for analysis.\")\n",
    "    else:\n",
    "        print(f\"Total paragraphs extracted: {len(paragraphs)}\")\n",
    "        if len(paragraphs) < 5:\n",
    "            print(\"Warning: Not enough paragraphs extracted. Try adjusting the paragraph splitting method.\")\n",
    "        \n",
    "        # Find most similar paragraph pairs\n",
    "        n_paragraphs = len(paragraphs)\n",
    "        most_similar = []\n",
    "\n",
    "        for i in range(n_paragraphs):\n",
    "            for j in range(i+1, n_paragraphs):\n",
    "                if similarity_matrix[i, j] > 0.75:  # Adjust threshold for meaningful similarity\n",
    "                    most_similar.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "        # Display top 3 most similar paragraph pairs\n",
    "        most_similar.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        if most_similar:\n",
    "            print(\"\\nMost similar sections (first 100 characters of each):\")\n",
    "            for i, j, sim in most_similar[:3]:\n",
    "                print(f\"\\nSimilarity: {sim:.2f}\")\n",
    "                print(f\"Paragraph {i}: {paragraphs[i][:100]}...\")\n",
    "                print(f\"Paragraph {j}: {paragraphs[j][:100]}...\")\n",
    "        else:\n",
    "            print(\"No highly similar paragraph pairs found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
