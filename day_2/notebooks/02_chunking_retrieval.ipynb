{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# ESG Document Chunking & Retrieval\n",
       "\n",
       "This notebook demonstrates how to build a document retrieval system for ESG reports using chunking and vector search.\n",
       "\n",
       "## Setup\n",
       "First, we'll import the necessary libraries and our custom modules."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import json\n",
       "import pandas as pd\n",
       "from pathlib import Path\n",
       "from IPython.display import display\n",
       "\n",
       "# Import our custom modules\n",
       "import sys\n",
       "sys.path.append('../scripts')\n",
       "from chunking import ESGDocumentChunker, TextChunk\n",
       "from embedding import ESGEmbeddingManager, EmbeddedChunk"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Document Chunking\n",
       "\n",
       "Let's start by loading our processed ESG report and splitting it into chunks."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load processed ESG data from Day 1\n",
       "with open('../data/processed_esg_data.json', 'r') as f:\n",
       "    esg_data = json.load(f)\n",
       "\n",
       "# Initialize chunker\n",
       "chunker = ESGDocumentChunker(\n",
       "    chunk_size=500,      # Target chunk size in words\n",
       "    chunk_overlap=50,    # Words to overlap between chunks\n",
       "    min_chunk_size=100   # Minimum chunk size to keep\n",
       ")\n",
       "\n",
       "# Process first document as example\n",
       "doc = esg_data[0]\n",
       "metadata = {\n",
       "    'company': doc['company'],\n",
       "    'year': doc['year'],\n",
       "    'source': doc['original_file']\n",
       "}\n",
       "\n",
       "# Try both chunking methods\n",
       "fixed_chunks = chunker.chunk_document(doc['cleaned_text'], metadata, method='fixed')\n",
       "section_chunks = chunker.chunk_document(doc['cleaned_text'], metadata, method='section')\n",
       "\n",
       "print(f\"Fixed-size chunks: {len(fixed_chunks)}\")\n",
       "print(f\"Section-based chunks: {len(section_chunks)}\")\n",
       "\n",
       "# Display example chunks\n",
       "print(\"\\nExample fixed-size chunk:\")\n",
       "print(fixed_chunks[0].text[:200], \"...\")\n",
       "print(\"\\nExample section-based chunk:\")\n",
       "print(section_chunks[0].text[:200], \"...\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Exercise 1: Chunk Size Analysis\n",
       "\n",
       "Analyze how different chunk sizes affect the quality of text segments."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Try different chunk sizes\n",
       "chunk_sizes = [200, 500, 1000]\n",
       "results = []\n",
       "\n",
       "for size in chunk_sizes:\n",
       "    chunker = ESGDocumentChunker(chunk_size=size)\n",
       "    chunks = chunker.chunk_document(doc['cleaned_text'], metadata)\n",
       "    \n",
       "    results.append({\n",
       "        'chunk_size': size,\n",
       "        'num_chunks': len(chunks),\n",
       "        'avg_length': sum(len(c.text.split()) for c in chunks) / len(chunks)\n",
       "    })\n",
       "\n",
       "pd.DataFrame(results)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Generating Embeddings\n",
       "\n",
       "Now let's convert our chunks into vector embeddings for semantic search."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize embedding manager\n",
       "embedding_manager = ESGEmbeddingManager()\n",
       "\n",
       "# Generate embeddings for our chunks\n",
       "texts = [chunk.text for chunk in fixed_chunks]\n",
       "embeddings = embedding_manager.generate_embeddings(texts)\n",
       "\n",
       "# Create embedded chunks\n",
       "embedded_chunks = [\n",
       "    EmbeddedChunk(\n",
       "        text=chunk.text,\n",
       "        embedding=embedding,\n",
       "        metadata=chunk.metadata\n",
       "    )\n",
       "    for chunk, embedding in zip(fixed_chunks, embeddings)\n",
       "]\n",
       "\n",
       "# Build search index\n",
       "embedding_manager.build_index(embedded_chunks)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Testing Document Retrieval\n",
       "\n",
       "Let's test our retrieval system with some ESG-related queries."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def search_and_display(query: str, k: int = 3):\n",
       "    \"\"\"Search for chunks and display results nicely.\"\"\"\n",
       "    results = embedding_manager.search(query, k=k)\n",
       "    \n",
       "    print(f\"Query: {query}\\n\")\n",
       "    for i, result in enumerate(results, 1):\n",
       "        print(f\"Result {i} (Score: {result['score']:.3f})\")\n",
       "        print(f\"Text: {result['text'][:200]}...\")\n",
       "        print(f\"Source: {result['metadata']['source']}\\n\")\n",
       "\n",
       "# Test some queries\n",
       "queries = [\n",
       "    \"carbon emissions reduction targets\",\n",
       "    \"board diversity and inclusion\",\n",
       "    \"environmental impact assessment\"\n",
       "]\n",
       "\n",
       "for query in queries:\n",
       "    search_and_display(query)\n",
       "    print(\"-\" * 80 + \"\\n\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Exercise 2: Query Analysis\n",
       "\n",
       "Try different types of queries and analyze the retrieval quality."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Your turn!\n",
       "# 1. Try queries with different phrasings\n",
       "# 2. Test specific ESG metric queries\n",
       "# 3. Compare results with different k values\n",
       "\n",
       "# Example:\n",
       "variations = [\n",
       "    \"What are the company's CO2 emissions?\",\n",
       "    \"carbon dioxide emissions data\",\n",
       "    \"CO2 reduction goals\"\n",
       "]\n",
       "\n",
       "for query in variations:\n",
       "    search_and_display(query, k=2)\n",
       "    print(\"-\" * 80 + \"\\n\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Saving the Index\n",
       "\n",
       "Finally, let's save our index and chunks for later use."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Save index and chunks\n",
       "output_dir = Path('../data')\n",
       "embedding_manager.save_index(\n",
       "    index_path=output_dir / 'faiss_index.bin',\n",
       "    chunks_path=output_dir / 'embedded_chunks.json'\n",
       ")\n",
       "\n",
       "print(\"Index and chunks saved successfully!\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": "3.12.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }